{"meta":{"title":"Alex的稻草小屋","subtitle":null,"description":"Jeden Tag gibt's die Möglichkeit eines Wunders.","author":"alex","url":"https://justlooks.github.io"},"pages":[],"posts":[{"title":"alertmanager 解析","slug":"alertmanager-解析","date":"2017-09-15T03:04:39.000Z","updated":"2017-09-15T05:59:14.000Z","comments":true,"path":"2017/09/15/alertmanager-解析/","link":"","permalink":"https://justlooks.github.io/2017/09/15/alertmanager-解析/","excerpt":"","text":"alertmanager可以接受prometheus报警，由prometheus来定义满足什么条件的发送告警到alertmanagerprometheus 通过启动选项”-alertmanager.url=http://alertmanager:9093/“ ,来指定将告警发送到alertmanager-config.file=/etc/prometheus/prometheus.yaml选项定义了prometheus的配置文件，这个配置文件中 rule_files: - &quot;/etc/prometheus-rules/*.rules&quot; 规定了在哪个目录下搜索报警规则,让我们具体来看一条报警规则的编写 ALERT NodeCPUUsage # 报警名字为NodeCPUUsage IF (100 - (avg by (instance) (irate(node_cpu{kubernetes_name=&quot;prometheus-node-exporter&quot;,mode=&quot;idle&quot;}[5m])) * 100)) &lt; 75 FOR 2m ＃上面的IF表达式状态为真持续2分钟才会报警 LABELS { severity=&quot;page&quot; ＃将标签附加在报警上,可以实现报警的归类 } ANNOTATIONS { SUMMARY = &quot;{{$labels.instance}}: High CPU usage detected&quot;, ＃告警简介 DESCRIPTION = &quot;{{$labels.instance}}: CPU usage is above 75% (current value is: {{ $value }})&quot; ＃告警详情 } 以上LABEL和ANNOTATION中的value 可以模板化，变量$labels保存了告警instance的所有label的key/value的集合，$value保存了告警的值详情此外一旦告警生成，prometheus也会保存告警的信息,下面的值为1表示告警触发，值为0表示告警inactive ALERTS{alertname=&quot;NodeCPUUsage&quot;,alertstate=&quot;firing&quot;,instance=&quot;10.165.97.219:9100&quot;,severity=&quot;page&quot;} ALERTS{alertname=&quot;NodeCPUUsage&quot;,alertstate=&quot;pending&quot;,instance=&quot;10.165.97.219:9100&quot;,severity=&quot;page&quot;} alertmanager 实现了在报警上加上通知频率，速度，以及静默报警依赖和添加概要,对于报警分组的实现，alertmanager通过一个route树所有的报警都会通过一棵路由树，路由的root节点未配置匹配规则，所有alert通过root节点，然后根据root节点下的子节点，子节点的节点来分类，通常continue设置为false，如果设置为true则当匹配当前节点后还会继续尝试去匹配兄弟节点,然后执行匹配节点的规定的动作。还有个功能是inhibit，当某个场景条件满足时高等级的报警会抑制低等级的报警,如下规则是当alertname相同时如果存在warning和critical报警后者会抑制前者发送 inhibit_rules: source_match: severity: ‘critical’target_match: severity: ‘warning’Apply inhibition if the alertname is the same.equal: [‘alertname’]","categories":[],"tags":[{"name":"kubernetes, monitor","slug":"kubernetes-monitor","permalink":"https://justlooks.github.io/tags/kubernetes-monitor/"}]},{"title":"kubernetes v1.7.5安装及calico网络配置","slug":"kubernetes-v1-7-5安装及calico网络配置","date":"2017-09-12T06:13:32.000Z","updated":"2017-09-12T08:52:55.000Z","comments":true,"path":"2017/09/12/kubernetes-v1-7-5安装及calico网络配置/","link":"","permalink":"https://justlooks.github.io/2017/09/12/kubernetes-v1-7-5安装及calico网络配置/","excerpt":"","text":"kubeadm,kubectl,kubelet安装因为集群原来是1.7.0版本，所以需要升级，升级包的来源可以通过第三方yum源，或者直接从源代码编译获得，这里为了简便期间，使用了aliyun的kubernetes的yum源如果之前没有安装过则需要查看下列网址，获取使用的端口信息 https://kubernetes.io/docs/setup/independent/install-kubeadm/ 另外升级是注意依赖包 kubernetes-cni和socat的版本指定阿里云的yum源获得最新的包 123456# vi /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0 升级kubeadm,kubectl,kubelet 三个安装包1# yum update kubeadm kubelet kubectl 初始化集群主节点上执行初始化123456789101112131415# cat kubeadm_config.yamlapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationapi: advertiseAddress: 10.161.233.80 bindPort: 6443networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/16 podSubnet: 10.68.0.0/16kubernetesVersion: v1.7.5#token: &lt;string&gt;#tokenTTL: 0# kubeadm init --config kubeadm_config.yaml 从节点加入集群1# kubeadm join --token 379dc2.1caa582e7e5f1530 10.161.233.80:6443 完毕后检查,因为这里没有配网络插件，所以node显示状态为NotReady,而且kube-dns也显示为pending状态1234567891011121314# kubectl get nodesNAME STATUS AGE VERSIONk8s-node1 NotReady 12s v1.7.5k8s-node2 NotReady 1m v1.7.5# kubectl get pods --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system etcd-k8s-node2 1/1 Running 0 15skube-system kube-apiserver-k8s-node2 1/1 Running 0 27skube-system kube-controller-manager-k8s-node2 1/1 Running 0 23skube-system kube-dns-2425271678-ctkl2 0/3 Pending 0 54skube-system kube-proxy-n428c 1/1 Running 0 54skube-system kube-proxy-p15jc 1/1 Running 0 27skube-system kube-scheduler-k8s-node2 1/1 Running 0 33s calico安装注意这里由于kubeadm启动的etcd的监听地址为127.0.0.1,所以要修改下配置文件，开放内网地址123# vi /etc/kubernetes/manifests/etcd.yaml - --listen-client-urls=http://10.161.233.80:2379,http://127.0.0.1:2379 - --advertise-client-urls=http://10.161.233.80:2379 做权限绑定以及安装calico12# kubectl apply -f https://docs.projectcalico.org/v2.5/getting-started/kubernetes/installation/rbac.yaml# kubectl apply -f https://docs.projectcalico.org/v2.5/getting-started/kubernetes/installation/hosted/calico.yaml 在执行calico.yaml文件前需要修改下内容，指定etcd地址,和ip pool地址段1234etcd_endpoints: &quot;http://10.161.233.80:2379&quot; - name: CALICO_IPV4POOL_CIDR value: &quot;10.68.0.0/16&quot; 安装calicoctl1234567# kubectl apply -f https://docs.projectcalico.org/v2.5/getting-started/kubernetes/installation/hosted/calicoctl.yaml# kubectl exec -ti -n kube-system calicoctl -- /calicoctl get profiles -o wideNAME TAGSk8s_ns.default k8s_ns.defaultk8s_ns.kube-public k8s_ns.kube-publick8s_ns.kube-system k8s_ns.kube-system","categories":[],"tags":[{"name":"kubernetes,calico","slug":"kubernetes-calico","permalink":"https://justlooks.github.io/tags/kubernetes-calico/"}]},{"title":"关于kubernetes的认证授权以及准入控制","slug":"关于kubernetes的认证授权以及准入控制","date":"2017-09-08T12:26:16.000Z","updated":"2017-09-11T01:57:55.000Z","comments":true,"path":"2017/09/08/关于kubernetes的认证授权以及准入控制/","link":"","permalink":"https://justlooks.github.io/2017/09/08/关于kubernetes的认证授权以及准入控制/","excerpt":"","text":"[root@k8s-master pki]# ls /etc/kubernetes/pkiapiserver.crt apiserver-kubelet-client.crt ca.crt front-proxy-ca.crt front-proxy-client.crt sa.keyapiserver.key apiserver-kubelet-client.key ca.key front-proxy-ca.key front-proxy-client.key sa.pub 其中ca.crt是作为根证书,内置ca公钥，用于验证某证书是否ca签发 ，我们可以看到当apiserver启动时会关联这个根证书[root@k8s-master pki]# grep — ‘—client-ca-file’ /etc/kubernetes/manifests/kube-apiserver.yaml - --client-ca-file=/etc/kubernetes/pki/ca.crt 另外两个apiserver启动时用于验证的文件,apiserver.crt是apiserver的证书文件，apiserver.key是apiserver的私钥文件[root@k8s-master pki]# grep — ‘—tls’ /etc/kubernetes/manifests/kube-apiserver.yaml - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key 验证 openssl verify -CAfile ca.crt apiserver.crtapiserver.crt: OK 因为apiserver已经开启了tls认证，所以通过内网访问apiserver会导致拒绝 curl -k https://10.132.41.234:6443/apicurl: (35) SSL connect error 如果要成功认证必须制作客户端证书产生客户端私钥 openssl genrsa -out legend.key 2048生成客户端签名请求 openssl req -new -key legend.key -subj “/CN=10.132.41.234” -out legend.csr用根证书和私钥签名生成客户端证书 openssl x509 -req -in legend.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out legend.crt -days 3650[root@legenddb1 tmp]# lsca.crt ca.key ca.srl legend.crt legend.csr legend.key 接下来apiserver-kubelet-client.crt和apiserver-kubelet-client.crt是什么呢？[root@k8s-master pki]# openssl verify -CAfile ca.crt apiserver-kubelet-client.crtapiserver-kubelet-client.crt: OK我们可以看到这个证书也是由根证书ca签名的查看kubectl的配置文件可以看到如下内容[root@k8s-master pki]# cat ~/.kube/configapiVersion: v1clusters: cluster: certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFM01EY3lOVEExTWpVeU1sb1hEVEkzTURjeU16QTFNalV5TWxvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTGphCnRhcFQ3ZVNoYmt1VzhNRHVPSVZ2bjBGQSt1bDhCOTZaYjAwVlJJUUNuYUNoaFV6TWFaWERYNjFuUnpnQXJJanQKZzlBeCtVaHJGSGZiVFIxZTZMMmR0QmtEK3VzNjNKTTdhVlFmNmNMbExzT3I0ZjJpQ2VYR0o1amJBdFhHcUE4ZQpXS1pJdzZlMmtVYXlNZllHZ1VtUkdPRURPVUFlYzFuMFRUUVFiSHRkOGk5NzVuWlc1NUllTlpBMTZTTmlmdzFyCjRyMEc3NFU3cVovZFhISTBMcFpWTDc1U05xN1JEZGx5dUZXQmg3YzV2RGdJTkZUbUlUVThRZFBSbHRaNm53M2cKbEdadEtmSUFLNzZ0STQ4VFFlOFl2SVRJMEdFaUtWTTVhOGFJeFhpY0pZTTlESXVWbVhlQno1QTZHUmxQNzlxOAo4WGU5eEhXck1hTVo3Ny9ZekhFQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFFKzJKdHJwczFqRWM5RGZ5UE84TnZzQ2RLcUEKTWRidHlsamNHV1d2eW5oTFh0dHpIRDh5MHhlSDRwQzBMbmhsUWVkekdjb3VwVlZIeklsdEpGZEdRUUZJS0h4WgphTU9YSm9LSTJwSk5BZFIyVEFhMXV6cVJXcTFBTE5uU0RLd3pYUm5yU01ZK21nN2Q3T3IyS2FwYmI0ZUtRcEZUCmJpV2dYeXZUSHl1U0NHYVRzWEZuVjIyY2kvcCtWT1cweEcwUVNwUXR0cFd2bHVxZlY2WGFFcmFOMXZjNXEyYkcKYVo1cTB6WVlKVGZzZERJU0p3cmtWSU8wRks2WVhTcHZaajRYeDBLZllZUEQ4R0tTTzNMczFrQ2NSbitMUkFvYwp1UE9wVDlKUFFBTjhnVXprdk1OeFlWNzlBVko1NlQwYlI5OGVHMWNVVWhEc0VsT2JWZ0srTE1KcDlJOD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= server: https://10.132.41.234:6443name: kubernetescontexts: context: cluster: kubernetes user: kubernetes-adminname: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kuberneteskind: Configpreferences: {}users: name: kubernetes-adminuser: client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM4akNDQWRxZ0F3SUJBZ0lJTTFwdnBxSnh6ZUl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB4TnpBM01qVXdOVEkxTWpKYUZ3MHhPREEzTWpVd05USTFNalphTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQW51SEpYdEs4MXBLQ1pRVzMKcGNpSVNzdnRkbVhpTjZPSDU5TWpqT01uQ0V3eFRKVGxaL2xZZEwxcWVYQU1sT2Fvb0Vub2tma1lPcWRQVDFyZAo4NTFhWms0SWVvZVpiR1dsYjlzTTQ5R1h6R3E2eXcxUW1QUDNnNHdvYWtwTFNMZ285V3JPeDJLMmVDRVhBYVNxCm0zQU5hS2JSa1RmQlRxUmY1K28rT3pXS3lQMmRrVTF2dy9YR1YxVWlVbkZPRzJQQklqK3VmMFYzWlptM0c5a2IKRFphZFB2cndDQlY5R1cwbDEzY1RTd2I5aWRTMWplWTdCQ1oxZ3Ixc2E3SC9VTUpycnNBNzhXWEFwbSt5UElkaApQdHJ3QzQvTksxRDFnUkl4dStZMFJ6ZWtZUmhnNlRnejRZeXFHUFlkZVpKTTk0VkFJbEVURGJtVWxZN2t4ck44Ck0rUWtJUUlEQVFBQm95Y3dKVEFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFLRkRWa2NOY3VGcEI2dUVEOUtRTVFDaE1SR1lBWlBEMVFWRQowVzl2cFp4L3g0dlhyS1R3amFXdFBwbGZOakZUY29sYTNsNzJZTkl2bU5ueGhmblFqVS9rUHlYZVdPZ3RWT3cwCjgrZ2lFVktzT1dya242V1dLOFZXUnBuV0xaTTd0bGtJbXRiMEpleGYzRm80b0d5UVYrNURHRE12QzhQU3A2NHoKTERhT1lROXRGOHJmS1VRVnF3Y2wzNWJzMlU0S2JyMHMzT1RRdG9KWkVaTEJNQVA2dGlRS1dJREdFMHpkQnJhcQpRaGJtdmszM0Ewc3Z5MFMwdldnOXRjeXFjODZwcENmOVUwVitNRmdJUDJsZ1NwQlBFcVBUckxhNVo4MEsvRGtYCmw1OEtwWHlob2N5SU5MYVdtTTdmZlJjWjhiREdGZHJWY1JHVnlhL2NORFdxaFZ1RFgwND0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= client-key-data: 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 通过base64 -d 解密我们可以看到 client-certificate-data 和client-key-data 正好对应的就是apiserver-kubelet-client.crt(?)和apiserver-kubelet-client.key内容而certificate-authority-data 正好是根证书的内容","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://justlooks.github.io/tags/kubernetes/"}]},{"title":"解决pod中容器实例运行时间不一致问题","slug":"解决pod中容器实例运行时间不一致问题","date":"2017-09-07T05:46:51.000Z","updated":"2017-09-07T05:50:22.000Z","comments":true,"path":"2017/09/07/解决pod中容器实例运行时间不一致问题/","link":"","permalink":"https://justlooks.github.io/2017/09/07/解决pod中容器实例运行时间不一致问题/","excerpt":"","text":"由于docker镜像在制作的时候会设置不同的时区，所以在运行pod也会产生时区不同，而对其中运行的应用造成一定影响12345[root@k8s-master tomcat]# date -RThu, 07 Sep 2017 09:39:39 +0800/etc # date -RThu, 07 Sep 2017 01:39:32 +0000 因为kubernetes物理节点的时区设置是统一的，我们需要做的就是将物理节点的时区挂载到pod中即可12# ls -l /etc/localtimelrwxrwxrwx 1 root root 33 Feb 24 2017 /etc/localtime -&gt; /usr/share/zoneinfo/Asia/Shanghai 将deploy的代码改成12345678910111213141516171819202122232425apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-state-metrics namespace: monitoringspec: replicas: 2 template: metadata: labels: app: kube-state-metrics spec: serviceAccountName: kube-state-metrics containers: - name: kube-state-metrics image: gcr.io/google_containers/kube-state-metrics:v0.5.0 ports: - containerPort: 8080 volumeMounts: - name: config-tz mountPath: /etc/localtime volumes: - name: config-tz hostPath: path: /usr/share/zoneinfo/Asia/Shanghai 验证结果12# kubectl exec prometheus-core-3823466589-j6g4b --namespace monitoring dateThu Sep 7 13:32:35 CST 2017","categories":[],"tags":[{"name":"kubernetes, zonetime","slug":"kubernetes-zonetime","permalink":"https://justlooks.github.io/tags/kubernetes-zonetime/"}]},{"title":"kubernetes - Garbage Collection","slug":"kubernetes-Garbage-Collection","date":"2017-08-31T06:19:24.000Z","updated":"2017-08-31T07:17:57.000Z","comments":true,"path":"2017/08/31/kubernetes-Garbage-Collection/","link":"","permalink":"https://justlooks.github.io/2017/08/31/kubernetes-Garbage-Collection/","excerpt":"","text":"Garbage Collection GC的用途就是清理那些原来有所有者但现在不再被所有的对象 举个例子来说，ReplicaSet是某群Pod的对象。对象的依赖者由metadata.ownerReferences定义。v1.6版本之后，ReplicationController, ReplicaSet, StatefulSet, DaemonSet, and Deployment会自动定义这个ownerReferences，当然也可以手动设置这个值进行指定。 级联删除级联删除的行为就是指删除某个对象的时候，如果其他相关对象的所有者是该对象，这些对象也会被删除。级联删除有两种模式后台级联删除和前台级联删除 后台级联删除 立即删除当前对象，尔作为被该对象所有的对象，gc会在后台进行清理 前台级联删除 根对象会进入一个““deletion in progress”的状态，在这个状态中该对象在REST API中仍然可见，该对象的deletionTimestamp会被设置，对象的metadata.finalizers的值包括foregroundDeletion。在进入这个状态后gc开始清理这些被根对象所有的对象，在这些被blocking的对象被删除后（带有ownerReference.blockOwnerDeletion=true的对象），根对象才会被清理，如果一个对象的ownerReferences被某个controller设置后，blockOwnerDeletion会按照规则被自动设置，不需要人为干预。 级联删除策略的控制通过设置所有者对象的deleteOptions.propagationPolicy域可以控制级联删除的策略，这些值包括 “Orphan”, “Foreground”, or “Background”。大多数controller资源的默认gc策略为”Orphan”。 下面的代码是background的方式清理被所有者对象background cascade delete1234kubectl proxy --port=8080curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset \\-d '&#123;\"kind\":\"DeleteOptions\",\"apiVersion\":\"v1\",\"propagationPolicy\":\"Background\"&#125;' \\-H \"Content-Type: application/json\"下面的代码是foreground的方式清理被所有者对象1234kubectl proxy --port=8080curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset \\-d &apos;&#123;&quot;kind&quot;:&quot;DeleteOptions&quot;,&quot;apiVersion&quot;:&quot;v1&quot;,&quot;propagationPolicy&quot;:&quot;Foreground&quot;&#125;&apos; \\-H &quot;Content-Type: application/json&quot; 下面是orphan方式 kubectl proxy —port=8080 curl -X DELETE localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/my-repset \\ -d ‘{“kind”:”DeleteOptions”,”apiVersion”:”v1”,”propagationPolicy”:”Orphan”}’ \\ -H “Content-Type: application/json” kubectl也支持级联删除方式，设置选项—cascade为true就可以自动删除被拥有的对象，如果留下被拥有的对象，则将该选项设置为false，默认值为true kubectl delete replicaset my-repset --cascade=false 当在deployment中使用级联删除时必须使用propagationPolicy: Foreground，来清理所有相关对象","categories":[],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://justlooks.github.io/tags/kubernetes/"}]},{"title":"Markdown 语法简介","slug":"测试","date":"2017-08-29T14:06:00.000Z","updated":"2017-08-31T03:28:01.000Z","comments":true,"path":"2017/08/29/测试/","link":"","permalink":"https://justlooks.github.io/2017/08/29/测试/","excerpt":"","text":"一级标题二级标题三级标题四级标题五级标题六级标题 引用 嵌套引用 列表1 list2 list1 list2 [ ] [X] 1print &quot;hello world!\\n&quot;; 斜体 粗体 my blog &#x6d;&#x79;&#64;&#x6d;&#x61;&#x69;&#x6c;&#46;&#x63;&#x6f;&#109; x^yR_{m \\times n} = U_{m \\times m} S_{m \\times n} V_{n \\times n}' st=>start: Start|past:>http://www.google.com[blank] e=>end: End|future:>http://www.google.com op1=>operation: My Operation|past op2=>operation: Stuff|current sub1=>subroutine: My Subroutine|invalid cond=>condition: Yes or No?|approved:>http://www.google.com c2=>condition: Good idea|rejected io=>inputoutput: catch something...|future st->op1(right)->cond cond(yes, right)->c2 cond(no)->sub1(left)->op1 c2(yes)->io->e c2(no)->op2->e{\"scale\":1,\"line-width\":2,\"line-length\":50,\"text-margin\":10,\"font-size\":12} var code = document.getElementById(\"flowchart-0-code\").value; var options = JSON.parse(decodeURIComponent(document.getElementById(\"flowchart-0-options\").value)); var diagram = flowchart.parse(code); diagram.drawSVG(\"flowchart-0\", options);","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2017-08-29T12:33:26.000Z","updated":"2017-08-29T12:33:26.000Z","comments":true,"path":"2017/08/29/hello-world/","link":"","permalink":"https://justlooks.github.io/2017/08/29/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}